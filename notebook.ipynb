{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = './pdfs/'\n",
    "\n",
    "embeddings_directory  = './embeddings/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA INGESTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='VT-ADL: A Vision Transformer Network for Image\n",
      "Anomaly Detection and Localization\n",
      "Pankaj Mishra\n",
      "University of Udine, Italy\n",
      "Email: mishra.pankaj@spes.uniud.itRiccardo Verk\n",
      "University of Udine, Italy\n",
      "Email: verk.riccardo@spes.uniud.itDaniele Fornasier\n",
      "beanTech srl, Italy\n",
      "Email: daniele.fornasier@beantech.it\n",
      "Claudio Piciarelli\n",
      "University of Udine, Italy\n",
      "Email: claudio.piciarelli@uniud.itGian Luca Foresti\n",
      "University of Udine, Italy\n",
      "Email: gianluca.foresti@uniud.it\n",
      "Abstract —We present a transformer-based image anomaly\n",
      "detection and localization network. Our proposed model is\n",
      "a combination of a reconstruction-based approach and patch\n",
      "embedding. The use of transformer networks helps preserving\n",
      "the spatial information of the embedded patches, which is later\n",
      "processed by a Gaussian mixture density network to localize the\n",
      "anomalous areas. In addition, we also publish BTAD, a real-world\n",
      "industrial anomaly dataset. Our results are compared with other\n",
      "state-of-the-art algorithms using publicly available datasets like\n",
      "MNIST and MVTec.\n",
      "Index Terms —Anomaly Detection, Anomaly segmentation,\n",
      "Vision transformer, Gaussian density approximation, Anomaly\n",
      "dataset\n",
      "I. I NTRODUCTION\n",
      "In computer vision, an anomaly is any image or image\n",
      "portion which exhibits signiﬁcant variation from the pre-\n",
      "deﬁned characteristics of normality. Anomaly Detection is thus\n",
      "the task of identifying these novel samples in supervised or\n",
      "unsupervised ways. A system which can perform this task in\n",
      "an intelligent way is hugely in demand, as its applications\n",
      "range from video surveillance [1] to defect segmentation [2],\n",
      "[3], inspection [2], quality control [4], medical imagining [5],\n",
      "ﬁnancial transactions [6] etc. As it can be seen from the\n",
      "examples, anomaly detection is particularly signiﬁcant in the\n",
      "industrial ﬁeld, where it can be used to automatically identify\n",
      "defective products.\n",
      "Recent efforts have been made to improve the anomaly\n",
      "detection task in the ﬁeld of deep learning. Most of the works\n",
      "try to learn the manifold of a single class representing normal\n",
      "data[7], using an encoding-decoding scheme, and their output\n",
      "is a classiﬁcation of the input image as either normal or\n",
      "anomaly, while fewer works deal with the task to segment\n",
      "the local anomalous region in an image[8]. Majorly, the\n",
      "methods either use a reconstruction-based approach, or learn\n",
      "the distribution of the latent features extracted by a pre-trained\n",
      "network or trained in end-to-end fashion.\n",
      "Motivated from the above facts and industrial needs, we\n",
      "developed a Vision-Transformer-based image anomaly detec-\n",
      "tion and localization network (VT-ADL), which learns the\n",
      "This work is partially supported by beanTech srl.\n",
      "Fig. 1. The three products of BTAD dataset. First column shows an example of\n",
      "normal images, second column shows anomalous images, third column shows\n",
      "the anomalous image with pixel-level ground truth labels, fourth column\n",
      "shows the predicted heat map by our proposed method.\n",
      "manifold of normal class data in a semi-supervised way, thus\n",
      "requiring only normal data in the training process. The vision\n",
      "transformer network model, recently proposed by Dosovitskiy\n",
      "et al. [9], is a network designed to work on image patches\n",
      "trying to preserve their positional information. In our work\n",
      "we show how an adapted transformer network can be used for\n",
      "anomaly localization using Gaussian Approximation [10], [11]\n",
      "of the latent features and also how different conﬁgurations can\n",
      "be tweaked to win some of the shortcomings of the vision\n",
      "transformer network. In addition to this, we are also pub-\n",
      "lishing a real-world industrial dataset (the beanTech Anomaly\n",
      "Detection dataset — BTAD) for the anomaly detection task.\n",
      "The dataset contains a total of 2830 real-world images of 3\n",
      "industrial products showcasing body and surface defects.\n",
      "II. R ELATED WORK\n",
      "Image-based anomaly detection is not a new topic in the\n",
      "industrial use cases, as it has been used in many inspection and\n",
      "quality control schemes, however is still under investigation\n",
      "with modern deep learning techniques. Historically, severalarXiv:2104.10036v1  [cs.CV]  20 Apr 2021' metadata={'source': './pdfs/VT-ADL.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_name = 'VT-ADL.pdf'\n",
    "\n",
    "loader = PyPDFLoader(directory_path + file_name)\n",
    "docs = loader.load()\n",
    "print(docs[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "final_docs = text_splitter.split_documents(docs)\n",
    "print(len(final_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLLAMA EMBEDDINGS AND CREATING VECTORSTOREDB(FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding = (\n",
    "    OllamaEmbeddings(model = \"llama3.2\")\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(final_docs, embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVING THE EMBEDDINGS\n",
    "db.save_local(embeddings_directory+\"VIT-ADL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING THE EMBEDDINGS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Load the FAISS index from the local directory\n",
    "embedding = OllamaEmbeddings(model=\"llama3.2\")\n",
    "db = FAISS.load_local(embeddings_directory+\"VIT-ADL\", embedding, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fig. 2. Left image: model overview. Image is split into patches, which are augmented with positional embedding. The resulting sequence is fed to the'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"vision transformer\"\n",
    "res = db.similarity_search(query)\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='llama3.2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "llama = Ollama(model=\"llama3.2\")\n",
    "\n",
    "llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'query'], input_types={}, partial_variables={}, template=\"\\n    Based on the following context, explain the concept of '{query}' if it is mentioned in the context.\\n    <context>\\n    {context}\\n    </context>\\n    If the context includes a section heading related to '{query}', focus on that part.\\n\"), additional_kwargs={})])\n",
       "| Ollama(model='llama3.2')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Based on the following context, explain the concept of '{query}' if it is mentioned in the context.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    If the context includes a section heading related to '{query}', focus on that part.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm=llama, prompt=prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=Ollama(model='llama3.2'), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000018E6C54F4D0>, search_kwargs={'k': 10}))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## CREATING A QA CHAIN\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10},)\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llama, \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "qa_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The method used in this paper for detecting and removing anomalies is a transformer-based framework that combines reconstruction and patch-based learning for image anomaly detection and localization.\n",
      "\n",
      "Here's a breakdown of how it works:\n",
      "\n",
      "1. The input images are first encoded using a transformer encoder, which produces encoded features.\n",
      "2. These encoded features are then summed into a reconstruction vector, which is used as the input to the decoder.\n",
      "3. The reconstructed output is compared with the original image to compute two loss functions:\n",
      "\t* Reconstruction loss (-LL): measures how well the model can reconstruct the original image from the encoded features.\n",
      "\t* Anomaly score (MSE or SSIM): measures the difference between the original image and the reconstructed output, which is used as a score for anomaly detection.\n",
      "4. The combined loss function is weighted to balance between the reconstruction loss and the anomaly score.\n",
      "\n",
      "To detect anomalies, the model uses a combination of two strategies:\n",
      "\n",
      "1. Reconstruction-based approach: This method detects anomalies at a global level by comparing the reconstructed output with the original image. Anomalies are detected when the reconstructed output deviates significantly from the original image.\n",
      "2. Patch-based learning: This method involves dividing the input image into patches and analyzing each patch separately to detect local anomalies.\n",
      "\n",
      "The model uses Gaussian approximation of the latent features to preserve positional information, which helps in anomaly localization.\n",
      "\n",
      "Overall, the proposed framework uses a combination of reconstruction and patch-based learning to detect and localize anomalies in images.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke(\"What is the method used in this paper to detect and remove anomaly. Explain it as well\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
